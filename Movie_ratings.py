# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DElwk9lwfV51pSGomHRdgjvUs0Ot4gUQ
"""

# Import essential libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load the Titanic dataset (replace 'titanic.csv' with your actual file path)
data = pd.read_csv('/content/Titanic-Dataset.csv')

# Display the first few rows
print(data.head())

# Check for missing values
print(data.isnull().sum())

# Fill missing values in 'Age' with the median age
data['Age'].fillna(data['Age'].median(), inplace=True)

# Fill missing values in 'Embarked' with the most common port
data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)

# Drop the 'Cabin' column due to many missing values
data.drop(['Cabin'], axis=1, inplace=True)

# Convert 'Sex' into numerical values
data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})

# Convert 'Embarked' using one-hot encoding
data = pd.get_dummies(data, columns=['Embarked'], drop_first=True)

# Drop unnecessary columns like 'Name' and 'Ticket'
data.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)

# Define features (X) and target variable (y)
X = data.drop('Survived', axis=1)
y = data['Survived']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print(f"Confusion Matrix:\n{conf_matrix}")
print(f"Classification Report:\n{class_report}")

# Count the number of survivors and non-survivors
survival_count = data['Survived'].value_counts()

# Display the results
print(f"Number of people who did not survive: {survival_count[0]}")
print(f"Number of people who survived: {survival_count[1]}")

# Visualize survival distribution
plt.figure(figsize=(6, 4))
sns.countplot(data['Survived'])
plt.title('Survival Distribution')
plt.xlabel('Survived (1 = Yes, 0 = No)')
plt.ylabel('Count')
plt.show()

# Feature importance
importances = model.feature_importances_
features = X.columns
feature_importance_df = pd.DataFrame({'Features': features, 'Importance': importances})

# Sort the features by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Features', data=feature_importance_df)
plt.title('Feature Importance in Random Forest Model')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Load dataset (adjust the file path as necessary)
data = pd.read_csv('/content/IMDb Movies India.csv')

# Inspect the DataFrame to check column names
print("Columns:", data.columns)  # View available column names
print(data.head())               # View first few rows of the dataset

# Strip leading/trailing spaces from column names if necessary
data.columns = data.columns.str.strip()

# Data cleaning: drop rows with missing values in the required columns
data.dropna(subset=['rating', 'genre', 'director', 'actors'], inplace=True)  # Adjust column names if needed

# OneHotEncoding categorical features (genre, director, and actors)
encoder = OneHotEncoder()
genre_encoded = encoder.fit_transform(data[['genre']]).toarray()
director_encoded = encoder.fit_transform(data[['director']]).toarray()
actors_encoded = encoder.fit_transform(data[['actors']]).toarray()

# Convert encoded arrays into DataFrames
genre_df = pd.DataFrame(genre_encoded, index=data.index)
director_df = pd.DataFrame(director_encoded, index=data.index)
actors_df = pd.DataFrame(actors_encoded, index=data.index)

# Combine all features into a single DataFrame
X = pd.concat([genre_df, director_df, actors_df], axis=1)
y = data['rating']  # Target variable (adjust column name if necessary)

# Train-test split (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model training using Random Forest Regressor
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model using Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# Optional: Output predictions for further inspection
output = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(output.head())